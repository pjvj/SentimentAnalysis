{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import math\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negations_dic = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
    "                \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
    "                \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "                \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "                \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
    "                \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "                \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "                \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "                \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                \"you've\": \"you have\", \"ve\": \"have\", \"favor\":\"favour\", \"favorite\":\"favourite\", \"color\":\"colour\",\n",
    "                \"behavior\": \"behaviour\", \"labor\": \"labour\", \"neighbor\": \"neighbour\", \"flavor\":\"flavour\"}\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    def split(self, text):\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def pos_tag(self, sentences):\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "    \n",
    "splitter = Splitter()\n",
    "tok = WordPunctTokenizer()\n",
    "postagger = POSTagger()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def review_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    lower_case=\"\"\n",
    "    for i in range(0,len(clean)):\n",
    "        if text[i].isalpha():\n",
    "            lower_case=lower_case+text[i].lower()\n",
    "        elif text[i].isdigit():\n",
    "            lower_case=lower_case\n",
    "        else:\n",
    "            lower_case=lower_case+text[i]\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    forpos= \" \".join(words).strip()\n",
    "    splitted_sentences = splitter.split(forpos)\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    altered_text=\"\"\n",
    "    for sentences in pos_tagged_sentences:\n",
    "        for word in sentences:\n",
    "            v=word[2]\n",
    "            for val in v:\n",
    "                if (val==\"JJ\" or val==\"JJR\" or val==\"JJS\" or val==\"VB\" or val==\"VBD\" or val==\"VBG\"\n",
    "                    or val==\"VBN\" or val==\"VBP\" or val==\"VBZ\" or val==\"RB\" or val==\"RBR\" or val==\"RBS\" ):\n",
    "                    altered_text=altered_text+stemmer.stem(word[0])+\" \"\n",
    "    return altered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review</td>\n",
       "      <td>sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText     rating\n",
       "0                                             review  sentiment\n",
       "1  One of the other reviewers has mentioned that ...   positive\n",
       "2  A wonderful little production. <br /><br />The...   positive\n",
       "3  I thought this was a wonderful way to spend ti...   positive\n",
       "4  Basically there's a family where a little boy ...   negative\n",
       "5  Petter Mattei's \"Love in the Time of Money\" is...   positive\n",
       "6  Probably my all-time favorite movie, a story o...   positive\n",
       "7  I sure would like to see a resurrection of a u...   positive\n",
       "8  This show was an amazing, fresh & innovative i...   negative\n",
       "9  Encouraged by the positive comments about this...   negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store imdb movie reviews file in pandas dataframes\n",
    "cols = ['reviewText','rating']\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\", header=None, names=cols)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean review file \n",
    "clean_review_texts = np.empty([1, 2], dtype=object)\n",
    "for i in range(0,len(df)):\n",
    "#     print(i)\n",
    "    clean_review_texts=np.append(clean_review_texts, np.array([[review_cleaner(df.iloc[i]['reviewText']),df.iloc[i]['rating']]]), axis=0)\n",
    "clean_review_texts=np.delete(clean_review_texts, 0, 0)\n",
    "clean_df = pd.DataFrame(clean_review_texts,columns=['text','target'])    \n",
    "clean_df.to_csv('imdbclean_review.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  40574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "my_df = pd.read_csv('imdbclean_review.csv',index_col=0)\n",
    "cvec = CountVectorizer(dtype='float32')\n",
    "#cvec.fit(my_df.text)\n",
    "cvec.fit_transform(my_df['text'].values.astype('U'))\n",
    "l=len(cvec.get_feature_names())\n",
    "print(\"Number of features: \",l)\n",
    "neg_doc_matrix = cvec.transform(my_df[my_df.target == 'negative'].text)\n",
    "pos_doc_matrix = cvec.transform(my_df[my_df.target == 'positive'].text)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray((neg_tf)))\n",
    "pos = np.squeeze(np.asarray((pos_tf)))\n",
    "\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df.head()\n",
    "term_freq_df.columns = ['negative','positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] +term_freq_df['positive']\n",
    "#document_matrix = cvec.transform(my_df.text)\n",
    "term_freq_df.to_csv('imdb_term_freq.csv',encoding='utf-8')\n",
    "\n",
    "term_freq_df_new = pd.read_csv('imdb_term_freq.csv',index_col=0)\n",
    "#print(term_freq_tfidf)\n",
    "lexicon_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\n",
    "lexicon_df.columns = ['negative', 'positive']\n",
    "for i in range(0,l):\n",
    "    tot=term_freq_df_new.iloc[i]['total']\n",
    "    nn=term_freq_df_new.iloc[i]['negative']\n",
    "    ps=term_freq_df_new.iloc[i]['positive']\n",
    "    if nn==0:\n",
    "        lexicon_df.iloc[i]['negative']=0\n",
    "    elif nn==tot:\n",
    "        lexicon_df.iloc[i]['negative']=2.5*nn\n",
    "    else:\n",
    "        lexicon_df.iloc[i]['negative']=1/(math.log((tot/nn),10))\n",
    "\n",
    "    if ps==0:\n",
    "        lexicon_df.iloc[i]['positive']=0\n",
    "    elif ps==tot:\n",
    "        lexicon_df.iloc[i]['positive']=2.5*ps\n",
    "    else:\n",
    "        lexicon_df.iloc[i]['positive']=1/(math.log((tot/ps),10))\n",
    "        #print(1/(math.log((tot/ps),10)))\n",
    "        #print(float(term_freq_tfidff.iloc[i]['positive']))\n",
    "\n",
    "lexicon_df.to_csv('imbd_lexicon.csv',encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "createfeed=pd.read_csv('imbd_lexicon.csv',index_col=0)\n",
    "columns=[\"Sno\",\"freq_neg\",\"freq_pos\",\"overall\"]\n",
    "my_df=my_df.fillna(\"are\")\n",
    "ok=len(my_df)\n",
    "#print(my_df.isnull())\n",
    "feature_values=[0,0]\n",
    "#print(feature_values)\n",
    "\n",
    "with open('imdb_feed.csv','w') as myfile:\n",
    "    wr = csv.writer(myfile,quoting=csv.QUOTE_NONE)\n",
    "    wr.writerow(columns)\n",
    "    for i in range(0,ok):\n",
    "        if i%(ok//2)==0:\n",
    "            print(i)\n",
    "        feature_values=[0,0]\n",
    "        text_for_comparison = my_df.iloc[i]['text']\n",
    "        if text_for_comparison not in (None, \" \",\"are\"):\n",
    "            words= text_for_comparison.split()\n",
    "            for j in range(0,len(words)):\n",
    "                if 'nan' not in words:\n",
    "                    feature_values = np.add(feature_values,[createfeed.loc[words[j]][0],createfeed.loc[words[j]][1]])\n",
    "                else:\n",
    "                    feature_values = np.add(feature_values,[0,0])\n",
    "                #print(feature_values)\n",
    "            feature_values = feature_values.tolist()\n",
    "            #print(feature_values)\n",
    "            feature_values.insert(0, i)\n",
    "#             new_lab=int(my_df.iloc[i]['label']/2)\n",
    "            new_lab=my_df.iloc[i]['target']\n",
    "            feature_values.insert(3, new_lab)\n",
    "            wr.writerow(feature_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for i in range(0,6000):\n",
    "#         feature_values=[0,0,0]\n",
    "#         text_for_comparison = my_df.iloc[i]['text']\n",
    "#         words= text_for_comparison.split()\n",
    "#         for j in range(0,len(words)):\n",
    "#             feature_values = np.add(feature_values,[createfeed.loc[words[j]][0],createfeed.loc[words[j]][1],createfeed.loc[words[j]][2]])\n",
    "#         feature_values = feature_values.tolist()\n",
    "#         feature_values.insert(0, i)\n",
    "#         new_lab=my_df.iloc[i]['target']\n",
    "#         feature_values.insert(4, new_lab)\n",
    "#         wr.writerow(feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras.utils import to_categorical\n",
    "with open('imdb_feed.csv') as f:\n",
    "    content = f.readlines()\n",
    "lines = np.array(content) \n",
    "num_of_instances = lines.size\n",
    "print(num_of_instances)\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, y_all = [], []\n",
    "for i in range(1, num_of_instances):\n",
    "    val = [0, 0, 0]\n",
    "    s, val[0], val[1], label = lines[i].split(\",\")\n",
    "    #print(newlabel)\n",
    "    final_val = np.array(val, 'float32')\n",
    "    class_label = keras.utils.to_categorical(label,2)\n",
    "    #class_label=class_label[0:3]\n",
    "    y_all.append(class_label)\n",
    "    x_all.append(final_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.array(x_all)\n",
    "print(np.shape(x_all))\n",
    "y_all = np.array(y_all)\n",
    "print(np.shape(y_all))\n",
    "print(y_all)\n",
    "print(len(x_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "ind_list = [i for i in range(len(x_all))]\n",
    "shuffle(ind_list)\n",
    "x_all  = x_all[ind_list,]\n",
    "y_all = y_all[ind_list,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "modelsentif=Sequential()\n",
    "\n",
    "modelsentif.add(Dense(6,input_shape=(3,)))\n",
    "modelsentif.add(Activation(\"relu\"))\n",
    "\n",
    "modelsentif.add(Dense(3))\n",
    "modelsentif.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsentif.compile(optimizer=keras.optimizers.Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "histsentif = modelsentif.fit(x_all, y_all, epochs=40, batch_size=512, validation_split=0.15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(histsentif.history['acc'])\n",
    "plt.plot(histsentif.history['val_acc'])\n",
    "plt.title('Sentiment Dataset Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(histsentif.history['loss'])\n",
    "plt.plot(histsentif.history['val_loss'])\n",
    "plt.title('Sentiment Dataset Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "modelsentif_json = modelsentif.to_json()\n",
    "with open(\"modelsentif.json\", \"w\") as json_file:\n",
    "    json_file.write(modelsentif_json)\n",
    "# serialize weights to HDF5\n",
    "modelsentif.save_weights(\"modelsentif.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('modelsentif.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"modelsentif.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "pred_list = []; actual_list = []\n",
    "predictions = loaded_model.predict(x_all)\n",
    "for i in predictions:\n",
    "    pred_list.append(np.argmax(i))\n",
    "for i in y_all:\n",
    "    actual_list.append(np.argmax(i))\n",
    "cnf_matrix=confusion_matrix(actual_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    print(len(classes))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "class_names =['negative','neutral','positive']\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix; Sentiment Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on actual input\n",
    "import pandas as pd\n",
    "createfeed=pd.read_csv('imdb_lexicon.csv',index_col=0)\n",
    "feature_values=[0,0]\n",
    "str = \"I am not enjoying it.\"\n",
    "str_result =  review_cleaner(str)\n",
    "words= str_result.split()\n",
    "words.insert(0, '')\n",
    "words.insert(0, '')\n",
    "print(words)\n",
    "for j in range(2,len(words)):\n",
    "    try:\n",
    "        if (words[j-1]==\"not\" and words[j-2]!=\"not\") or (words[j-1]!=\"not\" and words[j-2]==\"not\"):\n",
    "            feature_values = np.add(feature_values,[createfeed.loc[words[j]][1],createfeed.loc[words[j]][0]])\n",
    "        else:\n",
    "            feature_values = np.add(feature_values,[createfeed.loc[words[j]][0],createfeed.loc[words[j]][1]])\n",
    "    except:\n",
    "        continue\n",
    "feature_values = feature_values.tolist()\n",
    "test_case = np.array(feature_values)\n",
    "\n",
    "#print(np.shape(test_case))\n",
    "checker = []\n",
    "checker.append(test_case)\n",
    "checker = np.array(checker)\n",
    "#print(np.shape(checker))\n",
    "\n",
    "custom = modelsentif.predict(checker)\n",
    "print(custom[0])\n",
    "\n",
    "# Order: negative, neutral, positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
